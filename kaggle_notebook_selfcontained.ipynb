{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec55575",
   "metadata": {},
   "source": [
    "# Bethy: Self-contained Kaggle Notebook\n",
    "\n",
    "End-to-end breath sound classification without importing local `.py` files. All code (feature extraction, dataset, model, training, evaluation, prediction) lives in this notebook. Update the dataset path cell below if your Kaggle dataset name differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check and optional installs\n",
    "import os, sys, subprocess, json\n",
    "is_kaggle = os.path.exists('/kaggle')\n",
    "print(f\"Running on Kaggle: {is_kaggle}\")\n",
    "print(sys.version)\n",
    "if is_kaggle:\n",
    "    # Install any missing dependencies quietly\n",
    "    pkgs = ['pyyaml','tensorboard']\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', *pkgs], check=False)\n",
    "    print('Dependencies checked/installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e847550",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configure paths and hyperparameters. The cell below auto-detects a likely dataset path under `/kaggle/input`. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, pprint, itertools\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect dataset path - looks for folders containing .wav and .txt files\n",
    "candidate_dirs = [\n",
    "    Path('/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files'),\n",
    "    Path('/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database'),\n",
    "    Path('/kaggle/input/respiratory-sound-database/audio_and_txt_files'),\n",
    "    Path('/kaggle/input/respiratory-sound-database'),\n",
    "    Path('/kaggle/input/icbhi-respiratory-sound-database'),\n",
    "    Path('/kaggle/input/icbhi'),\n",
    "    Path('./data/icbhi')\n",
    "]\n",
    "\n",
    "data_dir = None\n",
    "for d in candidate_dirs:\n",
    "    if d.exists():\n",
    "        # Check if this directory or subdirectories contain .wav files\n",
    "        wav_files = list(d.glob('*.wav')) + list(d.glob('**/*.wav'))\n",
    "        if wav_files:\n",
    "            # Use the parent that directly contains .wav files, or the root\n",
    "            data_dir = str(d)\n",
    "            print(f\"Found {len(wav_files)} wav files in: {d}\")\n",
    "            break\n",
    "\n",
    "if data_dir is None:\n",
    "    data_dir = str(candidate_dirs[-1])  # fallback placeholder\n",
    "    print(f\"⚠️ No wav files found! Using fallback: {data_dir}\")\n",
    "    print(\"Available input directories:\")\n",
    "    if Path('/kaggle/input').exists():\n",
    "        for p in Path('/kaggle/input').iterdir():\n",
    "            print(f\"  - {p}\")\n",
    "\n",
    "config = {\n",
    "    'model': {\n",
    "        'cnn_channels': [64, 128, 256],\n",
    "        'rnn_hidden_size': 256,\n",
    "        'rnn_num_layers': 2,\n",
    "        'attention_dim': 128,\n",
    "        'dropout': 0.5,\n",
    "        'num_classes': 4\n",
    "    },\n",
    "    'features': {\n",
    "        'sample_rate': 16000,\n",
    "        'n_fft': 2048,\n",
    "        'hop_length': 512,\n",
    "        'n_mels': 128,\n",
    "        'n_mfcc': 40,\n",
    "        'duration': 8.0\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 1e-4,\n",
    "        'patience': 10,\n",
    "        'num_workers': 2\n",
    "    },\n",
    "    'augmentation': {\n",
    "        'time_stretch': True,\n",
    "        'pitch_shift': True,\n",
    "        'noise_injection': True,\n",
    "        'mixup': False\n",
    "    },\n",
    "    'paths': {\n",
    "        'data_dir': data_dir,\n",
    "        'checkpoint_dir': '/kaggle/working/checkpoints' if is_kaggle else './checkpoints',\n",
    "        'log_dir': '/kaggle/working/logs' if is_kaggle else './logs'\n",
    "    },\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Data directory: {config['paths']['data_dir']}\")\n",
    "print(f\"Device: {config['device']}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "pprint.pprint(config['training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f773b",
   "metadata": {},
   "source": [
    "## Imports and utilities\n",
    "\n",
    "Core imports plus a helper to set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20361c",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Mel-spectrograms + MFCCs using torchaudio; normalize to zero mean/unit variance and pad/trim to 8 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, sample_rate=16000, n_fft=2048, hop_length=512, n_mels=128, n_mfcc=40, duration=8.0):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.duration = duration\n",
    "        self.target_length = int(sample_rate * duration)\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
    "        )\n",
    "        self.mfcc_transform = torchaudio.transforms.MFCC(\n",
    "            sample_rate=sample_rate, n_mfcc=n_mfcc,\n",
    "            melkwargs={'n_fft': n_fft, 'hop_length': hop_length, 'n_mels': n_mels}\n",
    "        )\n",
    "\n",
    "    def load_audio(self, audio_path: str) -> torch.Tensor:\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        if waveform.shape[1] < self.target_length:\n",
    "            pad = self.target_length - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad))\n",
    "        else:\n",
    "            waveform = waveform[:, :self.target_length]\n",
    "        return waveform\n",
    "\n",
    "    def extract_mel_spectrogram(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        mel_spec = self.mel_spectrogram(waveform)\n",
    "        return torch.log(mel_spec + 1e-9)\n",
    "\n",
    "    def extract_mfcc(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mfcc_transform(waveform)\n",
    "\n",
    "    def extract_features(self, audio_path: str) -> dict:\n",
    "        waveform = self.load_audio(audio_path)\n",
    "        mel = self.extract_mel_spectrogram(waveform)\n",
    "        mfcc = self.extract_mfcc(waveform)\n",
    "        return {'waveform': waveform, 'mel_spectrogram': mel, 'mfcc': mfcc}\n",
    "\n",
    "    def normalize(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        mean = features.mean()\n",
    "        std = features.std()\n",
    "        return (features - mean) / (std + 1e-9)\n",
    "\n",
    "def compute_deltas(features: torch.Tensor, width: int = 9) -> torch.Tensor:\n",
    "    deltas = torch.zeros_like(features)\n",
    "    for t in range(width, features.shape[-1] - width):\n",
    "        deltas[..., t] = (features[..., t+1:t+width+1].sum(dim=-1) - features[..., t-width:t].sum(dim=-1)) / (2 * width)\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3f989",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "Simple waveform noise/gain and SpecAugment masks for mel-spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcacc1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class AudioAugmentation:\n",
    "    def __init__(self, sample_rate=16000, noise_factor=0.005, gain_range=(0.5,1.5)):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.noise_factor = noise_factor\n",
    "        self.gain_range = gain_range\n",
    "    def add_noise(self, waveform):\n",
    "        return waveform + torch.randn_like(waveform) * self.noise_factor\n",
    "    def random_gain(self, waveform):\n",
    "        return waveform * random.uniform(*self.gain_range)\n",
    "    def apply_random(self, waveform):\n",
    "        if random.random() > 0.5:\n",
    "            waveform = self.add_noise(waveform)\n",
    "        if random.random() > 0.5:\n",
    "            waveform = self.random_gain(waveform)\n",
    "        return waveform\n",
    "class SpecAugment:\n",
    "    def __init__(self, freq_mask_param=30, time_mask_param=40, num_masks=2):\n",
    "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param)\n",
    "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param)\n",
    "        self.num_masks = num_masks\n",
    "    def __call__(self, spectrogram: torch.Tensor) -> torch.Tensor:\n",
    "        for _ in range(self.num_masks):\n",
    "            spectrogram = self.freq_mask(spectrogram)\n",
    "            spectrogram = self.time_mask(spectrogram)\n",
    "        return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970de445",
   "metadata": {},
   "source": [
    "## Dataset and dataloaders\n",
    "\n",
    "ICBHI patient-wise split (60/20/20). Extract features, apply optional augmentations, return combined mel+MFCC tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ce9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICBHIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ICBHI dataset loader.\n",
    "    \n",
    "    Dataset structure:\n",
    "    - Each audio file (*.wav) has a corresponding annotation file (*.txt) with same name\n",
    "    - Annotation format per line: start_time end_time is_wheeze is_crackle\n",
    "    - Values are space/tab separated\n",
    "    \"\"\"\n",
    "    CLASS_MAPPING = {(0,0):0, (1,0):1, (0,1):2, (1,1):3}  # (wheeze, crackle) -> label\n",
    "    CLASS_NAMES = ['normal', 'wheeze', 'crackle', 'both']\n",
    "    \n",
    "    def __init__(self, data_dir: str, split='train', feature_extractor=None, augment=False, config=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.augment = augment and split == 'train'\n",
    "        self.feature_extractor = feature_extractor or FeatureExtractor(**config['features'])\n",
    "        self.config = config\n",
    "        if self.augment:\n",
    "            self.audio_aug = AudioAugmentation(sample_rate=self.feature_extractor.sample_rate)\n",
    "            self.spec_aug = SpecAugment()\n",
    "        self.samples = self._load_annotations()\n",
    "        self.class_weights = self._compute_class_weights()\n",
    "    \n",
    "    def _find_audio_files(self):\n",
    "        \"\"\"Find all .wav files in the dataset directory (searches common locations)\"\"\"\n",
    "        audio_files = []\n",
    "        search_paths = [\n",
    "            self.data_dir,\n",
    "            self.data_dir / 'audio',\n",
    "            self.data_dir / 'audio_and_txt_files',\n",
    "            self.data_dir / 'Respiratory_Sound_Database' / 'audio_and_txt_files',\n",
    "        ]\n",
    "        for search_path in search_paths:\n",
    "            if search_path.exists():\n",
    "                audio_files.extend(list(search_path.glob('*.wav')))\n",
    "        return list(set(audio_files))\n",
    "    \n",
    "    def _load_annotations(self):\n",
    "        \"\"\"Load annotations from individual .txt files (same name as .wav files)\"\"\"\n",
    "        samples = []\n",
    "        audio_files = self._find_audio_files()\n",
    "        \n",
    "        if not audio_files:\n",
    "            raise FileNotFoundError(f\"No .wav files found in {self.data_dir}. Check your dataset path.\")\n",
    "        \n",
    "        print(f\"Found {len(audio_files)} audio files\")\n",
    "        \n",
    "        for audio_path in audio_files:\n",
    "            txt_path = audio_path.with_suffix('.txt')\n",
    "            if not txt_path.exists():\n",
    "                continue\n",
    "            \n",
    "            filename = audio_path.stem\n",
    "            \n",
    "            # Format: start_time end_time is_wheeze is_crackle\n",
    "            try:\n",
    "                with open(txt_path, 'r') as f:\n",
    "                    for line_num, line in enumerate(f):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        parts = line.split()\n",
    "                        if len(parts) < 4:\n",
    "                            continue\n",
    "                        \n",
    "                        start_time = float(parts[0])\n",
    "                        end_time = float(parts[1])\n",
    "                        is_wheeze = int(parts[2])\n",
    "                        is_crackle = int(parts[3])\n",
    "                        \n",
    "                        label = self.CLASS_MAPPING[(is_wheeze, is_crackle)]\n",
    "                        \n",
    "                        samples.append({\n",
    "                            'audio_path': audio_path,\n",
    "                            'filename': filename,\n",
    "                            'start_time': start_time,\n",
    "                            'end_time': end_time,\n",
    "                            'crackles': is_crackle,\n",
    "                            'wheezes': is_wheeze,\n",
    "                            'label': label,\n",
    "                            'cycle_id': f\"{filename}_{line_num}\"\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error reading {txt_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not samples:\n",
    "            raise ValueError(f\"No valid samples found. Check annotation format: 'start_time end_time is_wheeze is_crackle'\")\n",
    "        \n",
    "        print(f\"Loaded {len(samples)} respiratory cycles from {len(audio_files)} files\")\n",
    "        return self._split_patientwise(samples)\n",
    "    \n",
    "    def _split_patientwise(self, samples):\n",
    "        \"\"\"Split by patient ID (first part of filename before underscore)\"\"\"\n",
    "        patient_ids = sorted({s['filename'].split('_')[0] for s in samples})\n",
    "        n_train = int(0.6 * len(patient_ids))\n",
    "        n_val = int(0.2 * len(patient_ids))\n",
    "        train_p = set(patient_ids[:n_train])\n",
    "        val_p = set(patient_ids[n_train:n_train + n_val])\n",
    "        test_p = set(patient_ids[n_train + n_val:])\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            pool = train_p\n",
    "        elif self.split == 'val':\n",
    "            pool = val_p\n",
    "        else:\n",
    "            pool = test_p\n",
    "        \n",
    "        filtered = [s for s in samples if s['filename'].split('_')[0] in pool]\n",
    "        print(f\"{self.split} split: {len(filtered)} samples from {len(pool)} patients\")\n",
    "        return filtered\n",
    "    \n",
    "    def _compute_class_weights(self):\n",
    "        if not self.samples:\n",
    "            return torch.ones(4)\n",
    "        labels = [s['label'] for s in self.samples]\n",
    "        counts = np.bincount(labels, minlength=4)\n",
    "        total = len(labels)\n",
    "        weights = total / (len(counts) * counts + 1e-6)\n",
    "        return torch.FloatTensor(weights)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        feats = self.feature_extractor.extract_features(str(s['audio_path']))\n",
    "        \n",
    "        if self.augment:\n",
    "            feats['waveform'] = self.audio_aug.apply_random(feats['waveform'])\n",
    "            feats['mel_spectrogram'] = self.feature_extractor.extract_mel_spectrogram(feats['waveform'])\n",
    "            feats['mel_spectrogram'] = self.spec_aug(feats['mel_spectrogram'])\n",
    "        \n",
    "        mel = self.feature_extractor.normalize(feats['mel_spectrogram'])\n",
    "        mfcc = self.feature_extractor.normalize(feats['mfcc'])\n",
    "        combined = torch.cat([mel, mfcc], dim=1)\n",
    "        \n",
    "        label = s['label']\n",
    "        meta = {\n",
    "            'filename': s['filename'],\n",
    "            'audio_path': str(s['audio_path']),\n",
    "            'crackles': s['crackles'],\n",
    "            'wheezes': s['wheezes'],\n",
    "            'start_time': s['start_time'],\n",
    "            'end_time': s['end_time']\n",
    "        }\n",
    "        return combined, label, meta\n",
    "\n",
    "\n",
    "def get_dataloaders(config):\n",
    "    feat = FeatureExtractor(**config['features'])\n",
    "    train_ds = ICBHIDataset(config['paths']['data_dir'], 'train', feat,\n",
    "                            augment=config['augmentation']['noise_injection'], config=config)\n",
    "    val_ds = ICBHIDataset(config['paths']['data_dir'], 'val', feat, augment=False, config=config)\n",
    "    test_ds = ICBHIDataset(config['paths']['data_dir'], 'test', feat, augment=False, config=config)\n",
    "    \n",
    "    def make_loader(ds, shuffle):\n",
    "        return DataLoader(ds, batch_size=config['training']['batch_size'],\n",
    "                          shuffle=shuffle, num_workers=config['training']['num_workers'], pin_memory=True)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, make_loader(train_ds, True), make_loader(val_ds, False), make_loader(test_ds, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49807bfd",
   "metadata": {},
   "source": [
    "## Metrics and early stopping\n",
    "\n",
    "Compute per-class metrics, macro averages, specificity, ICBHI score; simple early stopping helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcafeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "class MetricsCalculator:\n",
    "    CLASS_NAMES = ['normal','wheeze','crackle','both']\n",
    "    def __init__(self, num_classes=4):\n",
    "        self.num_classes = num_classes\n",
    "    def _specificity(self, cm):\n",
    "        spec = np.zeros(self.num_classes)\n",
    "        for i in range(self.num_classes):\n",
    "            tn = cm.sum() - cm[i,:].sum() - cm[:,i].sum() + cm[i,i]\n",
    "            fp = cm[:,i].sum() - cm[i,i]\n",
    "            spec[i] = tn / (tn + fp + 1e-9)\n",
    "        return spec\n",
    "    def calculate(self, y_true, y_pred, y_proba=None):\n",
    "        metrics = {}\n",
    "        metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "        metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(self.num_classes))\n",
    "        metrics['confusion_matrix'] = cm\n",
    "        spec = self._specificity(cm)\n",
    "        metrics['specificity_macro'] = spec.mean()\n",
    "        metrics['icbhi_score'] = (metrics['recall_macro'] + metrics['specificity_macro']) / 2\n",
    "        return metrics\n",
    "    def report(self, y_true, y_pred):\n",
    "        return classification_report(y_true, y_pred, target_names=self.CLASS_NAMES, zero_division=0)\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.0, mode='max'):\n",
    "        self.patience = patience; self.min_delta = min_delta; self.mode = mode\n",
    "        self.best = None; self.counter = 0; self.early_stop = False\n",
    "    def __call__(self, current):\n",
    "        if self.best is None:\n",
    "            self.best = current; return False\n",
    "        improved = current > self.best + self.min_delta if self.mode=='max' else current < self.best - self.min_delta\n",
    "        if improved:\n",
    "            self.best = current; self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True; return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e291bc",
   "metadata": {},
   "source": [
    "## Model: Hybrid CNN-RNN-Attention\n",
    "\n",
    "CNN blocks on spectrograms → BiLSTM → attention → MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, attention_dim):\n",
    "        super().__init__()\n",
    "        self.att = nn.Sequential(nn.Linear(hidden_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim,1))\n",
    "    def forward(self, x):\n",
    "        scores = self.att(x).squeeze(-1)\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), x).squeeze(1)\n",
    "        return context, weights\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        return self.drop(x)\n",
    "class HybridCNNRNNAttention(nn.Module):\n",
    "    def __init__(self, input_channels=1, cnn_channels=[64,128,256], rnn_hidden_size=256, rnn_num_layers=2, attention_dim=128, num_classes=4, dropout=0.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_ch = input_channels\n",
    "        for out_ch in cnn_channels:\n",
    "            layers.append(CNNBlock(in_ch, out_ch))\n",
    "            in_ch = out_ch\n",
    "        self.cnn_blocks = nn.ModuleList(layers)\n",
    "        self.cnn_output_channels = cnn_channels[-1]\n",
    "        self.lstm = nn.LSTM(input_size=self.cnn_output_channels, hidden_size=rnn_hidden_size, num_layers=rnn_num_layers, batch_first=True, dropout=dropout if rnn_num_layers>1 else 0, bidirectional=True)\n",
    "        self.attention = AttentionLayer(hidden_dim=rnn_hidden_size*2, attention_dim=attention_dim)\n",
    "        self.classifier = nn.Sequential(nn.Linear(rnn_hidden_size*2,512), nn.ReLU(), nn.Dropout(dropout), nn.Linear(512,256), nn.ReLU(), nn.Dropout(dropout), nn.Linear(256,num_classes))\n",
    "    def forward(self, x):\n",
    "        for block in self.cnn_blocks:\n",
    "            x = block(x)\n",
    "        x = x.permute(0,3,2,1).reshape(x.size(0), x.size(3), -1)\n",
    "        lstm_out,_ = self.lstm(x)\n",
    "        context, attn = self.attention(lstm_out)\n",
    "        logits = self.classifier(context)\n",
    "        return logits, attn\n",
    "    def predict_proba(self, x):\n",
    "        logits, _ = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "def create_model(config, device):\n",
    "    mcfg = config['model']\n",
    "    model = HybridCNNRNNAttention(input_channels=1, cnn_channels=mcfg['cnn_channels'], rnn_hidden_size=mcfg['rnn_hidden_size'], rnn_num_layers=mcfg['rnn_num_layers'], attention_dim=mcfg['attention_dim'], num_classes=mcfg['num_classes'], dropout=mcfg['dropout']).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992297e7",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Epoch-wise training/validation with class-weighted loss, LR scheduler, checkpointing, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ef432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    preds_all, labels_all = [], []\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "    for features, labels, _ in pbar:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(features)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward(); optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        preds_all.extend(preds.cpu().numpy()); labels_all.extend(labels.cpu().numpy())\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    avg_loss = total_loss/len(loader)\n",
    "    acc = np.mean(np.array(preds_all)==np.array(labels_all))\n",
    "    return avg_loss, acc, np.array(labels_all), np.array(preds_all)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    preds_all, labels_all, probas_all = [], [], []\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for features, labels, _ in pbar:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            logits, attn = model(features)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            probas = torch.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            preds_all.extend(preds.cpu().numpy()); labels_all.extend(labels.cpu().numpy()); probas_all.extend(probas.cpu().numpy())\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    avg_loss = total_loss/len(loader)\n",
    "    acc = np.mean(np.array(preds_all)==np.array(labels_all))\n",
    "    return avg_loss, acc, np.array(labels_all), np.array(preds_all), np.array(probas_all)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, metrics, path, is_best=False):\n",
    "    state = {'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'metrics': metrics}\n",
    "    torch.save(state, path)\n",
    "    if is_best:\n",
    "        best_path = Path(path).with_name(Path(path).stem + '_best.pth')\n",
    "        torch.save(state, best_path)\n",
    "\n",
    "def train(config):\n",
    "    device = torch.device(config['device'])\n",
    "    Path(config['paths']['checkpoint_dir']).mkdir(parents=True, exist_ok=True)\n",
    "    Path(config['paths']['log_dir']).mkdir(parents=True, exist_ok=True)\n",
    "    train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = get_dataloaders(config)\n",
    "    model = create_model(config, device)\n",
    "    class_weights = train_ds.class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['training']['learning_rate'], weight_decay=config['training']['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    early_stop = EarlyStopping(patience=config['training']['patience'], mode='max')\n",
    "    metrics_calc = MetricsCalculator()\n",
    "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[], 'val_icbhi':[]}\n",
    "    best_icbhi = -1\n",
    "    for epoch in range(1, config['training']['num_epochs']+1):\n",
    "        tr_loss, tr_acc, tr_y, tr_p = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "        va_loss, va_acc, va_y, va_p, va_proba = validate_epoch(model, val_loader, criterion, device, epoch)\n",
    "        va_metrics = metrics_calc.calculate(va_y, va_p, va_proba)\n",
    "        scheduler.step(va_loss)\n",
    "        history['train_loss'].append(tr_loss); history['val_loss'].append(va_loss); history['train_acc'].append(tr_acc); history['val_acc'].append(va_acc); history['val_icbhi'].append(va_metrics['icbhi_score'])\n",
    "        print(f\"Epoch {epoch}: train_loss={tr_loss:.4f} val_loss={va_loss:.4f} val_acc={va_acc:.4f} icbhi={va_metrics['icbhi_score']:.4f}\")\n",
    "        ckpt_path = str(Path(config['paths']['checkpoint_dir'])/f'checkpoint_epoch_{epoch}.pth')\n",
    "        is_best = va_metrics['icbhi_score'] > best_icbhi\n",
    "        if is_best: best_icbhi = va_metrics['icbhi_score']\n",
    "        save_checkpoint(model, optimizer, epoch, va_metrics, ckpt_path, is_best=is_best)\n",
    "        if early_stop(va_metrics['icbhi_score']):\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    print(f\"Best ICBHI score: {best_icbhi:.4f}\")\n",
    "    return model, history, (train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52554e",
   "metadata": {},
   "source": [
    "## Evaluation helper\n",
    "\n",
    "Evaluate a trained model on a dataloader; returns metrics, predictions, probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d347160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device, metrics_calc):\n",
    "    model.eval()\n",
    "    preds_all, labels_all, probas_all, attn_all, spec_all = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for features, labels, _ in tqdm(loader, desc='Eval', leave=False):\n",
    "            features = features.to(device)\n",
    "            logits, attn = model(features)\n",
    "            probas = torch.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            labels_all.extend(labels.numpy())\n",
    "            probas_all.extend(probas.cpu().numpy())\n",
    "            attn_all.extend(attn.cpu().numpy())\n",
    "            spec_all.extend(features[:,0,:,:].cpu().numpy())\n",
    "    metrics = metrics_calc.calculate(np.array(labels_all), np.array(preds_all), np.array(probas_all))\n",
    "    report = metrics_calc.report(np.array(labels_all), np.array(preds_all))\n",
    "    return {'metrics': metrics, 'report': report, 'labels': np.array(labels_all), 'preds': np.array(preds_all), 'probas': np.array(probas_all), 'attention': attn_all, 'spectrograms': spec_all}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa971b",
   "metadata": {},
   "source": [
    "## Prediction helper\n",
    "\n",
    "Convenience class for single-file and batch predictions with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d76026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreathSoundPredictor:\n",
    "    def __init__(self, model_path: str, config: dict, device: torch.device):\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.class_names = ICBHIDataset.CLASS_NAMES\n",
    "        self.fe = FeatureExtractor(**config['features'])\n",
    "        self.model = create_model(config, device)\n",
    "        ckpt = torch.load(model_path, map_location=device)\n",
    "        state = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt\n",
    "        self.model.load_state_dict(state)\n",
    "        self.model.eval()\n",
    "    def _prepare(self, audio_path: str):\n",
    "        feats = self.fe.extract_features(audio_path)\n",
    "        mel = self.fe.normalize(feats['mel_spectrogram'])\n",
    "        mfcc = self.fe.normalize(feats['mfcc'])\n",
    "        combined = torch.cat([mel, mfcc], dim=1).unsqueeze(0).to(self.device)\n",
    "        return combined\n",
    "    def predict_single(self, audio_path: str):\n",
    "        with torch.no_grad():\n",
    "            x = self._prepare(audio_path)\n",
    "            logits, attn = self.model(x)\n",
    "            probas = torch.softmax(logits, dim=1)[0]\n",
    "            pred_idx = int(probas.argmax().item())\n",
    "            return {\n",
    "                'predicted_class': self.class_names[pred_idx],\n",
    "                'predicted_index': pred_idx,\n",
    "                'confidence': float(probas[pred_idx].item()),\n",
    "                'probabilities': {name: float(probas[i].item()) for i,name in enumerate(self.class_names)},\n",
    "                'attention': attn[0].cpu().numpy(),\n",
    "            }\n",
    "    def predict_batch(self, audio_paths: list, batch_size: int = 8):\n",
    "        results = []\n",
    "        for i in range(0, len(audio_paths), batch_size):\n",
    "            batch = audio_paths[i:i+batch_size]\n",
    "            tensors = []\n",
    "            valid_paths = []\n",
    "            for p in batch:\n",
    "                try:\n",
    "                    tensors.append(self._prepare(p))\n",
    "                    valid_paths.append(p)\n",
    "                except Exception as e:\n",
    "                    results.append({'audio_path': p, 'error': str(e)})\n",
    "            if not tensors:\n",
    "                continue\n",
    "            x = torch.cat(tensors, dim=0)\n",
    "            with torch.no_grad():\n",
    "                logits, attn = self.model(x)\n",
    "                probas = torch.softmax(logits, dim=1)\n",
    "                preds = probas.argmax(dim=1)\n",
    "            for j,pth in enumerate(valid_paths):\n",
    "                pred_idx = int(preds[j].item())\n",
    "                results.append({\n",
    "                    'audio_path': pth,\n",
    "                    'predicted_class': self.class_names[pred_idx],\n",
    "                    'predicted_index': pred_idx,\n",
    "                    'confidence': float(probas[j,pred_idx].item()),\n",
    "                    'probabilities': {name: float(probas[j,i].item()) for i,name in enumerate(self.class_names)}\n",
    "                })\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9aea2",
   "metadata": {},
   "source": [
    "## Quickstart: train (optional)\n",
    "\n",
    "Uncomment and run to train. For a fast smoke test, lower epochs and batch size. Training can be slow on CPU; enable GPU in Kaggle settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run a short training session\n",
    "## config['training']['num_epochs'] = 3\n",
    "## config['training']['batch_size'] = 16\n",
    "## model, history, loaders = train(config)\n",
    "## train_loader, val_loader, test_loader = loaders\n",
    "## print('Training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce6864",
   "metadata": {},
   "source": [
    "## Quickstart: evaluate a checkpoint\n",
    "\n",
    "Point `best_ckpt_path` to your saved checkpoint (e.g., `/kaggle/working/checkpoints/checkpoint_epoch_XX_best.pth`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation (set your checkpoint path)\n",
    "best_ckpt_path = '/kaggle/working/checkpoints/checkpoint_epoch_1_best.pth'  # change me\n",
    "## Uncomment to run evaluation\n",
    "## if Path(best_ckpt_path).exists():\n",
    "##     _, _, _, train_loader, val_loader, test_loader = get_dataloaders(config)\n",
    "##     model = create_model(config, torch.device(config['device']))\n",
    "##     ckpt = torch.load(best_ckpt_path, map_location=torch.device(config['device']))\n",
    "##     state = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt\n",
    "##     model.load_state_dict(state)\n",
    "##     metrics_calc = MetricsCalculator()\n",
    "##     results = evaluate_model(model, test_loader, torch.device(config['device']), metrics_calc)\n",
    "##     metrics_calc.print_metrics = lambda m, prefix='': print(prefix, m)\n",
    "##     print(results['report'])\n",
    "## else:\n",
    "##     print('Checkpoint not found:', best_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a086e",
   "metadata": {},
   "source": [
    "## Quickstart: single prediction\n",
    "\n",
    "Use `BreathSoundPredictor` to classify one file or a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9625d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction (set your paths)\n",
    "## predictor = BreathSoundPredictor(model_path=best_ckpt_path, config=config, device=torch.device(config['device']))\n",
    "## audio_path = '/kaggle/input/your-audio.wav'\n",
    "## if Path(audio_path).exists():\n",
    "##     result = predictor.predict_single(audio_path)\n",
    "##     print(result)\n",
    "## else:\n",
    "##     print('Audio file not found:', audio_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
