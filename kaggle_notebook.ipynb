{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1067f8bf",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install required packages that might not be available in Kaggle by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Kaggle\n",
    "import os\n",
    "is_kaggle = os.path.exists('/kaggle')\n",
    "print(f\"Running on Kaggle: {is_kaggle}\")\n",
    "\n",
    "# Install any missing dependencies\n",
    "if is_kaggle:\n",
    "    !pip install -q pyyaml tensorboard\n",
    "    print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372111a",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57eda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path if running from uploaded files\n",
    "if is_kaggle:\n",
    "    # If you uploaded the code as a dataset, add it to path\n",
    "    # sys.path.append('/kaggle/input/bethy-code')\n",
    "    pass\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10680c9b",
   "metadata": {},
   "source": [
    "## 3. Verify Dataset Path\n",
    "\n",
    "Make sure the dataset is properly mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List input datasets\n",
    "if is_kaggle:\n",
    "    print(\"Available input datasets:\")\n",
    "    !ls -la /kaggle/input/\n",
    "    \n",
    "    # Update this path based on your dataset name\n",
    "    # Common paths:\n",
    "    # - /kaggle/input/respiratory-sound-database/\n",
    "    # - /kaggle/input/icbhi-respiratory-sound-database/\n",
    "    \n",
    "    print(\"\\nChecking for audio files and annotations...\")\n",
    "    # Adjust this path based on your actual dataset structure\n",
    "    data_paths = [\n",
    "        \"/kaggle/input/respiratory-sound-database\",\n",
    "        \"/kaggle/input/icbhi-respiratory-sound-database\",\n",
    "        \"/kaggle/input/icbhi\"\n",
    "    ]\n",
    "    \n",
    "    dataset_path = None\n",
    "    for path in data_paths:\n",
    "        if os.path.exists(path):\n",
    "            dataset_path = path\n",
    "            print(f\"Found dataset at: {path}\")\n",
    "            !ls -la {path}\n",
    "            break\n",
    "    \n",
    "    if dataset_path is None:\n",
    "        print(\"⚠️ Dataset not found! Please check the path.\")\n",
    "else:\n",
    "    dataset_path = \"./data/icbhi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5c789",
   "metadata": {},
   "source": [
    "## 4. Load Configuration\n",
    "\n",
    "Load the Kaggle-specific configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config_path = 'config_kaggle.yaml' if is_kaggle else 'config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update data path with the actual dataset location\n",
    "if dataset_path:\n",
    "    config['paths']['data_dir'] = dataset_path\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Data directory: {config['paths']['data_dir']}\")\n",
    "print(f\"Checkpoint directory: {config['paths']['checkpoint_dir']}\")\n",
    "print(f\"Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"Number of epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"Device: {config['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841da87",
   "metadata": {},
   "source": [
    "## 5. Quick Data Exploration\n",
    "\n",
    "Let's explore the dataset structure and verify everything is correctly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import ICBHIDataset\n",
    "from src.features import FeatureExtractor\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor(**config['features'])\n",
    "\n",
    "# Load a small sample of the dataset\n",
    "print(\"Loading training dataset...\")\n",
    "train_dataset = ICBHIDataset(\n",
    "    data_dir=config['paths']['data_dir'],\n",
    "    split='train',\n",
    "    feature_extractor=feature_extractor,\n",
    "    augment=False,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Class names: {train_dataset.CLASS_NAMES}\")\n",
    "print(f\"Class weights: {train_dataset.class_weights}\")\n",
    "\n",
    "# Check class distribution\n",
    "labels = [sample['label'] for sample in train_dataset.samples]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(\"\\nClass distribution:\")\n",
    "for idx, count in zip(unique, counts):\n",
    "    print(f\"  {train_dataset.CLASS_NAMES[idx]}: {count} ({count/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe675e80",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data\n",
    "\n",
    "Visualize a sample audio file and its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4eeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import plot_spectrogram, plot_class_distribution\n",
    "\n",
    "# Get a sample\n",
    "sample_idx = 0\n",
    "features, label, metadata = train_dataset[sample_idx]\n",
    "\n",
    "print(f\"Sample: {metadata['filename']}\")\n",
    "print(f\"True label: {train_dataset.CLASS_NAMES[label]}\")\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "\n",
    "# Plot mel-spectrogram\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(features[0, :128, :].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title(f'Mel-Spectrogram: {train_dataset.CLASS_NAMES[label]}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot MFCC\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(features[0, 128:, :].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('MFCC')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot class distribution\n",
    "plot_class_distribution(\n",
    "    np.array(labels),\n",
    "    train_dataset.CLASS_NAMES,\n",
    "    title='Training Set Class Distribution'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b691488",
   "metadata": {},
   "source": [
    "## 7. Create Model\n",
    "\n",
    "Initialize the hybrid CNN-RNN-Attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfff259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import create_model\n",
    "\n",
    "device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = create_model(config, device)\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b057eb",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "\n",
    "Train the model using the training script. This will:\n",
    "- Train for multiple epochs\n",
    "- Validate after each epoch\n",
    "- Save best model based on ICBHI score\n",
    "- Apply early stopping if no improvement\n",
    "- Log metrics to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31de9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs(config['paths']['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(config['paths']['log_dir'], exist_ok=True)\n",
    "\n",
    "# For quick testing, reduce epochs\n",
    "# config['training']['num_epochs'] = 5  # Uncomment for quick test\n",
    "\n",
    "# Run training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Import and run training function\n",
    "from train import train\n",
    "import argparse\n",
    "\n",
    "# Create a simple args object\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.config = config_path\n",
    "        self.data_dir = None\n",
    "        self.checkpoint_dir = None\n",
    "        self.resume = None\n",
    "\n",
    "args = Args()\n",
    "train(config, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1495e",
   "metadata": {},
   "source": [
    "## 9. Load Best Model and Evaluate\n",
    "\n",
    "Load the best saved model and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d643881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.metrics import MetricsCalculator\n",
    "from src.visualize import plot_confusion_matrix\n",
    "\n",
    "# Find the best model checkpoint\n",
    "checkpoint_files = list(Path(config['paths']['checkpoint_dir']).glob('*_best.pth'))\n",
    "if checkpoint_files:\n",
    "    best_model_path = str(checkpoint_files[0])\n",
    "    print(f\"Loading best model from: {best_model_path}\")\n",
    "else:\n",
    "    # Use latest checkpoint\n",
    "    checkpoint_files = sorted(Path(config['paths']['checkpoint_dir']).glob('checkpoint_epoch_*.pth'))\n",
    "    if checkpoint_files:\n",
    "        best_model_path = str(checkpoint_files[-1])\n",
    "        print(f\"Loading latest checkpoint: {best_model_path}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found!\")\n",
    "        best_model_path = None\n",
    "\n",
    "if best_model_path:\n",
    "    # Load model\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from epoch {checkpoint['epoch']}\")\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_dataset = ICBHIDataset(\n",
    "        data_dir=config['paths']['data_dir'],\n",
    "        split='test',\n",
    "        feature_extractor=feature_extractor,\n",
    "        augment=False,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probas = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels, metadata in test_loader:\n",
    "            features = features.to(device)\n",
    "            logits, _ = model(features)\n",
    "            probas = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probas.extend(probas.cpu().numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probas = np.array(all_probas)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics_calc = MetricsCalculator()\n",
    "    metrics = metrics_calc.calculate_metrics(all_labels, all_preds, all_probas)\n",
    "    \n",
    "    # Print results\n",
    "    metrics_calc.print_metrics(metrics, prefix=\"Test Set \")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(\n",
    "        metrics['confusion_matrix'],\n",
    "        test_dataset.CLASS_NAMES,\n",
    "        title='Test Set Confusion Matrix'\n",
    "    )\n",
    "    \n",
    "    plot_confusion_matrix(\n",
    "        metrics['confusion_matrix'],\n",
    "        test_dataset.CLASS_NAMES,\n",
    "        title='Test Set Confusion Matrix (Normalized)',\n",
    "        normalize=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9c5ca",
   "metadata": {},
   "source": [
    "## 10. Sample Predictions with Visualization\n",
    "\n",
    "Make predictions on some test samples and visualize the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc697e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import plot_attention_on_spectrogram, plot_prediction_confidence\n",
    "\n",
    "# Get some test samples\n",
    "num_samples = 4\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples*3))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    features, label, metadata = test_dataset[i]\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        features_batch = features.unsqueeze(0).to(device)\n",
    "        logits, attention_weights = model(features_batch)\n",
    "        probas = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # Get data\n",
    "    mel_spec = features[0, :128, :].numpy()\n",
    "    attn = attention_weights[0].cpu().numpy()\n",
    "    probs = probas[0].cpu().numpy()\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    axes[i, 0].imshow(mel_spec, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[i, 0].set_title(f'Sample {i+1}: {metadata[\"filename\"][:20]}...')\n",
    "    axes[i, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Plot attention weights\n",
    "    axes[i, 1].bar(range(len(attn)), attn, color='steelblue')\n",
    "    axes[i, 1].set_title('Attention Weights')\n",
    "    axes[i, 1].set_ylabel('Weight')\n",
    "    \n",
    "    # Plot prediction confidence\n",
    "    colors = ['green' if j == pred else 'red' if j == label else 'gray' for j in range(4)]\n",
    "    axes[i, 2].bar(test_dataset.CLASS_NAMES, probs, color=colors)\n",
    "    axes[i, 2].set_title(f'True: {test_dataset.CLASS_NAMES[label]} | Pred: {test_dataset.CLASS_NAMES[pred]}')\n",
    "    axes[i, 2].set_ylabel('Probability')\n",
    "    axes[i, 2].set_ylim(0, 1)\n",
    "    axes[i, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f76a1c",
   "metadata": {},
   "source": [
    "## 11. Save Model for Submission\n",
    "\n",
    "Save the trained model and configuration for later use or submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "output_path = '/kaggle/working/bethy_final_model.pth'\n",
    "if best_model_path:\n",
    "    import shutil\n",
    "    shutil.copy(best_model_path, output_path)\n",
    "    print(f\"Model saved to: {output_path}\")\n",
    "    print(f\"Model size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Save configuration\n",
    "config_output_path = '/kaggle/working/config_final.yaml'\n",
    "with open(config_output_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "print(f\"Configuration saved to: {config_output_path}\")\n",
    "\n",
    "print(\"\\n✅ Training completed! Model ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec11d6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Dataset loading and exploration\n",
    "2. ✅ Feature extraction (mel-spectrograms + MFCCs)\n",
    "3. ✅ Model training with CNN-RNN-Attention architecture\n",
    "4. ✅ Evaluation with comprehensive metrics (ICBHI Score, F1, Precision, Specificity)\n",
    "5. ✅ Attention mechanism visualization\n",
    "6. ✅ Model saving for deployment\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different hyperparameters\n",
    "- Try data augmentation variations\n",
    "- Test on your own audio files\n",
    "- Deploy as a web application"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
